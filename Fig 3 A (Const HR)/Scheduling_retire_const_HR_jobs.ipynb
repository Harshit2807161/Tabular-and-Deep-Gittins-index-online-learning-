{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be4efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "from mpmath import mp\n",
    "import time \n",
    "import copy \n",
    "from tqdm import tqdm \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7959d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_with_counter(file_name):\n",
    "    row_counter = 0\n",
    "    rowwise_lists = []\n",
    "\n",
    "    # Open and read the CSV file\n",
    "    with open(file_name, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  # Skip header row\n",
    "\n",
    "        # Iterate through each row in the CSV file\n",
    "        for row in reader:\n",
    "            row_as_int = [int(value) for value in row]  # Convert each value to integer\n",
    "            rowwise_lists.append(row_as_int)\n",
    "            row_counter += 1  # Increment the counter\n",
    "    return rowwise_lists\n",
    "\n",
    "file_name = \"C:\\Intern\\Gittin\\'s plots\\All codes directory\\Fig 3 A (Const HR)\\jobsizes_constHR.csv\"  # Replace with the actual file name\n",
    "rowwise_lists = load_csv_with_counter(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c96707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_v_values(Q_values_action1, Q_values_action2):\n",
    "    V_values = np.zeros((2,10))\n",
    "    for i in range(2):\n",
    "        for j in range(10):\n",
    "            V_values[i][j] = max(Q_values_action1[i][j],Q_values_action2[i][j])  # Use keepdims=True to maintain the 2D shape\n",
    "    return V_values\n",
    "\n",
    "V_true = np.array([[ 0,         0,          0,          0,          0,          0,\n",
    "   0,          0,          0,          0        ],\n",
    "   [5.20609088, 10.98257709, 16.18277733, 21.10806607, 26.14558744, 30.49934127,\n",
    "  36.64971727, 41.42251637, 45.25001465, 50.77376517]])\n",
    "# V_true = [9, 8.15241315, 7.47595723, 6.90163656, 6.49113047]\n",
    "\n",
    "def bellman_relative_error(V_approx, V_true):\n",
    "    nonzero_indices = V_true != 0\n",
    "    if np.any(nonzero_indices):\n",
    "        #relative_errors = np.abs((V_approx[nonzero_indices] - V_true[nonzero_indices]) / V_true[nonzero_indices])\n",
    "        relative_errors = np.abs((V_approx[nonzero_indices] - V_true[nonzero_indices]))\n",
    "        return np.mean(relative_errors)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af50e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class envir():\n",
    "    def __init__(self):\n",
    "        self.phi = 9\n",
    "        self.p1=[]\n",
    "        for i in range(10):\n",
    "            self.p1.append(0.05*(i+1))\n",
    "    def step(self,s,state_dict,task,jobsize):\n",
    "            s_old = s[task]\n",
    "            reward = [0]*10\n",
    "            next_state = copy.copy(s)\n",
    "            if(state_dict[f\"{task}\"]==jobsize[task] or s_old==0):\n",
    "                s_new = 0\n",
    "            else:\n",
    "                state_dict[f\"{task}\"] += 1\n",
    "                s_new = s_old\n",
    "            next_state[task] = s_new\n",
    "            if s[task]!=0 and next_state[task] == 0:\n",
    "                reward[task] = 1\n",
    "            elif s[task]==0:\n",
    "                reward[task] = -10000\n",
    "            else:\n",
    "                reward[task] = 0\n",
    "            return next_state,state_dict,reward[task]\n",
    "    \n",
    "    def getjobsize(self, job_counter):\n",
    "        jobsize = rowwise_lists[job_counter]\n",
    "        return jobsize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,alpha,gamma):\n",
    "        self.Q_values = np.zeros((2,2,10))\n",
    "        self.alpha = alpha\n",
    "        #self.c= c\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def activate_task_eps_greedy(self,s,M,epsilon):\n",
    "            wl= []\n",
    "            p = np.array([0,0,0,0,0,0,0,0,0,0])\n",
    "            if np.random.random() < epsilon:\n",
    "                for i in range(len(p)):\n",
    "                    if s[i]!=0:\n",
    "                        wl.append(i)\n",
    "                arm_to_pull = np.random.choice(wl,1)[0]\n",
    "                p[arm_to_pull] = 1\n",
    "                return arm_to_pull\n",
    "            else:\n",
    "                p2 = {}\n",
    "                for i in range(len(p)):\n",
    "                    if s[i]!=0:\n",
    "                        p2[i] = M[s[i]][i]\n",
    "                max_key = max(p2, key=p2.get)\n",
    "                p[max_key] = 1\n",
    "                return max_key\n",
    "                \n",
    "    def check_best_action(self,state):\n",
    "        ind = 0\n",
    "        for i,_ in reversed(list(enumerate(state))):\n",
    "            if state[i] == 1:\n",
    "                ind = i\n",
    "                break\n",
    "        return ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f5581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1903/2500 [11:40<04:31,  2.20it/s]"
     ]
    }
   ],
   "source": [
    "Q_values = np.zeros((2,2,10))\n",
    "\n",
    "def select_task(s,M):\n",
    "        k = []\n",
    "        for i in range(10):\n",
    "            k.append(M[s[i]][i])\n",
    "        max_index = np.argmax(k)  # Get the index of the maximum value\n",
    "        return max_index\n",
    "\n",
    "class game():\n",
    "    start_time = time.time()\n",
    "    M = np.zeros((2,10))\n",
    "    '''for i in range(100):\n",
    "        for j in range(9):\n",
    "            M[i][j] = 0'''\n",
    "    c0 = 0\n",
    "    hist01 = []\n",
    "    ti = []\n",
    "    c1 = 0\n",
    "    m = 0\n",
    "    m1 = 0\n",
    "    ct = 0\n",
    "    F = np.zeros((2,10))\n",
    "    env = envir()\n",
    "    hist = []\n",
    "    hist2 =[]\n",
    "    hist3 =[]\n",
    "    hist4 =[]\n",
    "    hist5 =[]\n",
    "    hist6 = []\n",
    "    trials = 2500\n",
    "    tries = 0\n",
    "    cumm_rew = []\n",
    "    BRE = []\n",
    "    plt_wrong_actions = []\n",
    "    cumm_wrong_steps = []\n",
    "    agent = Agent(alpha = 0.1,gamma = 0.99)\n",
    "    eps = 1\n",
    "    for trial_no in tqdm(range(trials)):\n",
    "        s = np.array([1,1,1,1,1,1,1,1,1,1])\n",
    "        #Calculating Q values over state space for a given arm for the given M vector through Q learning\n",
    "        #for step in range(2):\n",
    "        #print(tries,trial_no)\n",
    "        tries = 0 \n",
    "        episode_rew = 0\n",
    "        eps = eps*0.9985\n",
    "        state_dict = {\"0\":1,\"1\":1,\"2\":1,\"3\":1,\"4\":1,\"5\":1,\"6\":1,\"7\":1,\"8\":1,\"9\":1}\n",
    "        jobsize = env.getjobsize(trial_no)\n",
    "        '''\n",
    "        if((trial_no%5)==0):\n",
    "            agent.alpha = agent.alpha-0.0002\n",
    "        '''\n",
    "        #print(f\"Trial no: {trial_no}\")\n",
    "        while (s[0]!=0 or s[1]!=0 or s[2]!=0 or s[3]!=0 or s[4]!=0 or s[5]!=0 or s[6]!=0 or s[7]!=0 or s[8]!=0 or s[9]!=0):\n",
    "            if len(cumm_rew)==0:\n",
    "                cumm_rew.append(episode_rew)\n",
    "            else:\n",
    "                cumm_rew.append(cumm_rew[-1]+episode_rew)\n",
    "            #print(s)\n",
    "            tries += 1 \n",
    "            task_eps = agent.activate_task_eps_greedy(s,M,eps)\n",
    "            task = task_eps\n",
    "            task_opt = agent.check_best_action(s)\n",
    "            current_time = time.time()-start_time\n",
    "            next_state, state_dict,R = env.step(copy.copy(s),state_dict,task,jobsize)\n",
    "            #print(next_state)\n",
    "            episode_rew += R\n",
    "            if task_eps != task_opt:\n",
    "                cumm_wrong_steps.append(1)\n",
    "            else:\n",
    "                cumm_wrong_steps.append(0)\n",
    "            plt_wrong_actions.append(np.mean(cumm_wrong_steps)*100)\n",
    "            Q0 = np.zeros((2,10))\n",
    "            for i in range(2):\n",
    "                for j in range(10):\n",
    "                    Q0[i][j] = agent.Q_values[i][i][j]\n",
    "            V_values = calculate_v_values(Q0,M)\n",
    "            BRE.append(bellman_relative_error(V_values,V_true))\n",
    "\n",
    "            #print(task)\n",
    "            #print(next_state)\n",
    "            #print('r',R)\n",
    "            for k in range(2):\n",
    "                Q_values[s[task]][k][task] += agent.alpha*(R+agent.gamma*(max(M[k][task],copy.copy(Q_values[next_state[task]][k][task])))-Q_values[s[task]][k][task])    \n",
    "            s= copy.copy(next_state) \n",
    "\n",
    "          #Algorithm for stochastic approximation\n",
    "\n",
    "          #Calculating F\n",
    "            for i in range(2):\n",
    "                F[i][task] = -M[i][task]+Q_values[i][i][task]\n",
    "                #print('F',F[i][task])\n",
    "                #print('m',M[i][task])\n",
    "                #print('q',Q_values[i][i][task])\n",
    "    \n",
    "          #Print values\n",
    "            #print(\"Iteration no\",t)\n",
    "            #print(\"F\",F)\n",
    "\n",
    "          #Update M\n",
    "            for i in range(2):\n",
    "                M[i][task] = M[i][task] + 0.2*(F[i][task])       \n",
    "            hist01.append(0.01*M[1][5])\n",
    "            ti.append(current_time)\n",
    "\n",
    "          #Stopping criteria\n",
    "          #For M[i]\n",
    "            '''if(t>=10):\n",
    "                if(F[0]==0 and (histV0[len(histV0)-1]-histV0[len(histV0)-5]<0.1)):\n",
    "                    ct=1;\n",
    "                    m=M[0]   \n",
    "          #For algorithm\n",
    "            print(\" \")\n",
    "            histm0.append(M[0])\n",
    "            histm1.append(M[1])\n",
    "            if ((np.linalg.norm(F)<.9) or t>=9000) and (t>=2 and np.linalg.norm(V-Vold)<0.1):\n",
    "                print(\"Gittin's index for state 0 is\",0.2*min(m,(M[0])))\n",
    "                print(\"Gittin's index for state 1 is\",0.2*(M[1]))\n",
    "                break'''\n",
    "            '''hist.append(Q_values[1][1][1])\n",
    "            hist2.append(Q_values[2][2][1])\n",
    "            hist3.append(Q_values[3][3][1])\n",
    "            hist4.append(M[1][1])\n",
    "            hist5.append(M[2][1])\n",
    "            hist6.append(M[3][1])'''\n",
    "    for i in range(2):\n",
    "        for j in range(10):\n",
    "            Q0[i][j] = agent.Q_values[i][i][j]\n",
    "    V_values = calculate_v_values(Q0,M)\n",
    "    plt.title('Gittins index of state 1 of job 6 vs runtime plot',fontsize='xx-large')\n",
    "    plt.xlabel('Time', fontsize = 'xx-large')\n",
    "    plt.ylabel('Gittins index',fontsize = 'xx-large')\n",
    "    plt.plot(ti,hist01,'-',c='red')\n",
    "    '''plt.plot(ti,hist2,'-',c='cyan')\n",
    "    plt.plot(ti,hist3,'-',c='green')\n",
    "    plt.plot(ti,hist4,'-',c='yellow')\n",
    "    plt.plot(ti,hist5,'-',c='black')\n",
    "    plt.plot(ti,hist6,'-',c='grey')'''\n",
    "    plt.show()\n",
    "    #print(M[0][:])\n",
    "    print(M)\n",
    "    filename = 'C:\\\\Intern\\\\percent_wrong_QGI_constHR.csv'\n",
    "\n",
    "    # Writing to CSV file\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['percent_wrong'])  # Writing the header\n",
    "        for value in plt_wrong_actions:\n",
    "            writer.writerow([value])  # Writing each value in a new row\n",
    "\n",
    "    filename = 'C:\\\\Intern\\\\BRE_QGI_constHR.csv'\n",
    "\n",
    "    # Writing to CSV file\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['BRE'])  # Writing the header\n",
    "        for value in BRE:\n",
    "            writer.writerow([value])  # Writing each value in a new row\n",
    "\n",
    "    filename = 'C:\\\\Intern\\\\cumm_rew_QGI_constHR.csv'\n",
    "    \n",
    "    # Writing to CSV file\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['cumm_rew'])  # Writing the header\n",
    "        for value in cumm_rew:\n",
    "            writer.writerow([value])  # Writing each value in a new row     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b29f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dream11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
